{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-12T16:49:45.370807Z",
     "start_time": "2023-12-12T16:49:45.308381Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HomeTeamExpenditure</th>\n",
       "      <th>HomeTeamIncome</th>\n",
       "      <th>HomeTeamBalance</th>\n",
       "      <th>AwayTeamExpenditure</th>\n",
       "      <th>AwayTeamIncome</th>\n",
       "      <th>AwayTeamBalance</th>\n",
       "      <th>home_rest_time</th>\n",
       "      <th>away_rest_time</th>\n",
       "      <th>HomeFTHG_all_avg_25</th>\n",
       "      <th>AwayFTHG_all_avg_25</th>\n",
       "      <th>...</th>\n",
       "      <th>D_Away</th>\n",
       "      <th>L_Away</th>\n",
       "      <th>GF_Away</th>\n",
       "      <th>GA_Away</th>\n",
       "      <th>GD_Away</th>\n",
       "      <th>Pts_Away</th>\n",
       "      <th>HomeElo</th>\n",
       "      <th>AwayElo</th>\n",
       "      <th>EloDiff</th>\n",
       "      <th>FTR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-12.00</td>\n",
       "      <td>34.08</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-34.08</td>\n",
       "      <td>98</td>\n",
       "      <td>98</td>\n",
       "      <td>1.24</td>\n",
       "      <td>1.04</td>\n",
       "      <td>...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>-12.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>1486.372150</td>\n",
       "      <td>1471.813088</td>\n",
       "      <td>14.559062</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.75</td>\n",
       "      <td>2.65</td>\n",
       "      <td>-4.10</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.10</td>\n",
       "      <td>2.10</td>\n",
       "      <td>98</td>\n",
       "      <td>98</td>\n",
       "      <td>1.32</td>\n",
       "      <td>0.88</td>\n",
       "      <td>...</td>\n",
       "      <td>13.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>1472.842921</td>\n",
       "      <td>1583.004052</td>\n",
       "      <td>-110.161131</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.88</td>\n",
       "      <td>0.38</td>\n",
       "      <td>-13.50</td>\n",
       "      <td>10.50</td>\n",
       "      <td>1.35</td>\n",
       "      <td>-9.15</td>\n",
       "      <td>98</td>\n",
       "      <td>98</td>\n",
       "      <td>1.52</td>\n",
       "      <td>1.68</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1450.497956</td>\n",
       "      <td>1489.364296</td>\n",
       "      <td>-38.866341</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9.08</td>\n",
       "      <td>6.90</td>\n",
       "      <td>-2.18</td>\n",
       "      <td>31.50</td>\n",
       "      <td>11.80</td>\n",
       "      <td>-19.70</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>1.24</td>\n",
       "      <td>0.84</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>1499.163815</td>\n",
       "      <td>1688.999370</td>\n",
       "      <td>-189.835554</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>34.55</td>\n",
       "      <td>0.23</td>\n",
       "      <td>-34.33</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.08</td>\n",
       "      <td>-0.33</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>1.92</td>\n",
       "      <td>2.16</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>-9.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>1590.870126</td>\n",
       "      <td>1516.163605</td>\n",
       "      <td>74.706521</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5615</th>\n",
       "      <td>94.00</td>\n",
       "      <td>169.20</td>\n",
       "      <td>75.20</td>\n",
       "      <td>153.20</td>\n",
       "      <td>44.60</td>\n",
       "      <td>-108.60</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>1.04</td>\n",
       "      <td>1.24</td>\n",
       "      <td>...</td>\n",
       "      <td>14.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>1548.891535</td>\n",
       "      <td>1700.032741</td>\n",
       "      <td>-151.141206</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5616</th>\n",
       "      <td>467.80</td>\n",
       "      <td>269.40</td>\n",
       "      <td>-198.40</td>\n",
       "      <td>64.85</td>\n",
       "      <td>10.09</td>\n",
       "      <td>-54.76</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>1.12</td>\n",
       "      <td>1.48</td>\n",
       "      <td>...</td>\n",
       "      <td>14.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>1563.118212</td>\n",
       "      <td>1602.142634</td>\n",
       "      <td>-39.024422</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5617</th>\n",
       "      <td>100.35</td>\n",
       "      <td>195.90</td>\n",
       "      <td>95.55</td>\n",
       "      <td>68.47</td>\n",
       "      <td>52.60</td>\n",
       "      <td>-15.87</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>2.12</td>\n",
       "      <td>1.36</td>\n",
       "      <td>...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1654.830595</td>\n",
       "      <td>1540.074128</td>\n",
       "      <td>114.756466</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5618</th>\n",
       "      <td>137.56</td>\n",
       "      <td>156.40</td>\n",
       "      <td>18.84</td>\n",
       "      <td>40.50</td>\n",
       "      <td>82.80</td>\n",
       "      <td>42.30</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>1.48</td>\n",
       "      <td>1.76</td>\n",
       "      <td>...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>-23.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>1569.364767</td>\n",
       "      <td>1484.013329</td>\n",
       "      <td>85.351438</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5619</th>\n",
       "      <td>203.40</td>\n",
       "      <td>55.34</td>\n",
       "      <td>-148.06</td>\n",
       "      <td>241.10</td>\n",
       "      <td>114.50</td>\n",
       "      <td>-126.60</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>1.84</td>\n",
       "      <td>0.92</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>1678.597156</td>\n",
       "      <td>1834.557445</td>\n",
       "      <td>-155.960289</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5620 rows Ã— 86 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      HomeTeamExpenditure  HomeTeamIncome  HomeTeamBalance  \\\n",
       "0                   12.00            0.00           -12.00   \n",
       "1                    6.75            2.65            -4.10   \n",
       "2                   13.88            0.38           -13.50   \n",
       "3                    9.08            6.90            -2.18   \n",
       "4                   34.55            0.23           -34.33   \n",
       "...                   ...             ...              ...   \n",
       "5615                94.00          169.20            75.20   \n",
       "5616               467.80          269.40          -198.40   \n",
       "5617               100.35          195.90            95.55   \n",
       "5618               137.56          156.40            18.84   \n",
       "5619               203.40           55.34          -148.06   \n",
       "\n",
       "      AwayTeamExpenditure  AwayTeamIncome  AwayTeamBalance  home_rest_time  \\\n",
       "0                   34.08            0.00           -34.08              98   \n",
       "1                    0.00            2.10             2.10              98   \n",
       "2                   10.50            1.35            -9.15              98   \n",
       "3                   31.50           11.80           -19.70              99   \n",
       "4                    0.40            0.08            -0.33             100   \n",
       "...                   ...             ...              ...             ...   \n",
       "5615               153.20           44.60          -108.60               7   \n",
       "5616                64.85           10.09           -54.76               7   \n",
       "5617                68.47           52.60           -15.87               8   \n",
       "5618                40.50           82.80            42.30               7   \n",
       "5619               241.10          114.50          -126.60               8   \n",
       "\n",
       "      away_rest_time  HomeFTHG_all_avg_25  AwayFTHG_all_avg_25  ...  D_Away  \\\n",
       "0                 98                 1.24                 1.04  ...     9.0   \n",
       "1                 98                 1.32                 0.88  ...    13.0   \n",
       "2                 98                 1.52                 1.68  ...     8.0   \n",
       "3                 99                 1.24                 0.84  ...     8.0   \n",
       "4                100                 1.92                 2.16  ...     8.0   \n",
       "...              ...                  ...                  ...  ...     ...   \n",
       "5615               7                 1.04                 1.24  ...    14.0   \n",
       "5616               7                 1.12                 1.48  ...    14.0   \n",
       "5617               6                 2.12                 1.36  ...     7.0   \n",
       "5618               8                 1.48                 1.76  ...    12.0   \n",
       "5619               8                 1.84                 0.92  ...     5.0   \n",
       "\n",
       "      L_Away  GF_Away  GA_Away  GD_Away  Pts_Away      HomeElo      AwayElo  \\\n",
       "0       17.0     35.0     47.0    -12.0      45.0  1486.372150  1471.813088   \n",
       "1        8.0     66.0     38.0     28.0      64.0  1472.842921  1583.004052   \n",
       "2       16.0     49.0     53.0     -4.0      50.0  1450.497956  1489.364296   \n",
       "3        6.0     67.0     30.0     37.0      80.0  1499.163815  1688.999370   \n",
       "4       15.0     48.0     57.0     -9.0      53.0  1590.870126  1516.163605   \n",
       "...      ...      ...      ...      ...       ...          ...          ...   \n",
       "5615     5.0     68.0     33.0     35.0      71.0  1548.891535  1700.032741   \n",
       "5616     9.0     58.0     46.0     12.0      59.0  1563.118212  1602.142634   \n",
       "5617    16.0     55.0     53.0      2.0      52.0  1654.830595  1540.074128   \n",
       "5618    18.0     34.0     57.0    -23.0      36.0  1569.364767  1484.013329   \n",
       "5619     5.0     94.0     33.0     61.0      89.0  1678.597156  1834.557445   \n",
       "\n",
       "         EloDiff  FTR  \n",
       "0      14.559062    1  \n",
       "1    -110.161131    0  \n",
       "2     -38.866341    1  \n",
       "3    -189.835554    0  \n",
       "4      74.706521    2  \n",
       "...          ...  ...  \n",
       "5615 -151.141206    1  \n",
       "5616  -39.024422    0  \n",
       "5617  114.756466    1  \n",
       "5618   85.351438    0  \n",
       "5619 -155.960289    0  \n",
       "\n",
       "[5620 rows x 86 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "drive_mount = False\n",
    "if drive_mount:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive/')\n",
    "    DATA_DIR = \"/content/drive/MyDrive/Colab_Notebooks/COMP0036/datasets\"\n",
    "else:\n",
    "    DATA_DIR = \"../datasets\"\n",
    "EPL_TRAINING_DATA_CSV = DATA_DIR + \"/epl-training-processed.csv\"\n",
    "\n",
    "\n",
    "# Read the CSV file\n",
    "epl_df = pd.read_csv(EPL_TRAINING_DATA_CSV)\n",
    "epl_df.drop(['Date', 'HomeTeam', 'AwayTeam'], axis=1, inplace=True)\n",
    "\n",
    "epl_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b1347ed90ab1d9",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6d6ef68d4884f492",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-13T14:32:35.471115Z",
     "start_time": "2023-12-13T14:28:00.002801Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Logistic Regression Parameters: {'classifier__C': 0.1, 'classifier__multi_class': 'multinomial'}\n",
      "Logistic Regression Best CV Accuracy: 0.5053\n",
      "Best Random Forest Parameters: {'classifier__max_depth': 15, 'classifier__min_samples_split': 5, 'classifier__n_estimators': 200}\n",
      "Random Forest Best CV Accuracy: 0.5190\n",
      "Best SVM Parameters: {'classifier__C': 0.1, 'classifier__kernel': 'rbf'}\n",
      "SVM Best CV Accuracy: 0.5096\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.metrics import make_scorer, accuracy_score\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "USE_XGBOOST = False\n",
    "if USE_XGBOOST:\n",
    "    from xgboost import XGBClassifier\n",
    "\n",
    "# Sample DataFrame\n",
    "df = epl_df.copy()\n",
    "\n",
    "# Drop rows with NaN values\n",
    "df = df.dropna()\n",
    "\n",
    "# Encode the 'FTR' column\n",
    "target_column = 'FTR'\n",
    "\n",
    "# Features and Target\n",
    "X = df.drop(target_column, axis=1)\n",
    "X = scaler.fit_transform(X)\n",
    "y = df[target_column]\n",
    "\n",
    "# Define 5-fold cross-validation\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Pipeline that includes SMOTE and the classifier\n",
    "def build_pipeline(classifier, classifier_params):\n",
    "    return ImbPipeline([\n",
    "        ('smote', SMOTE()),\n",
    "        ('classifier', classifier(**classifier_params))\n",
    "    ])\n",
    "\n",
    "# Parameter grid for Logistic Regression\n",
    "logistic_params = {'classifier__multi_class': ['multinomial'], 'classifier__C': [0.1, 1, 10]}\n",
    "\n",
    "# Parameter grid for Random Forest\n",
    "forest_params = {'classifier__n_estimators': [100, 200, 300], 'classifier__max_depth': [5, 10, 15], 'classifier__min_samples_split': [2, 5, 10]}\n",
    "\n",
    "# Parameter grid for SVM\n",
    "svm_params = {'classifier__C': [0.1, 1, 10], 'classifier__kernel': ['linear', 'rbf']}\n",
    "\n",
    "logistic_smote_pipeline = build_pipeline(LogisticRegression, {'max_iter': 1000})\n",
    "rf_smote_pipeline = build_pipeline(RandomForestClassifier, {'random_state': 42})\n",
    "svm_smote_pipeline = build_pipeline(SVC, {'probability': True})\n",
    "\n",
    "# Grid search for each model\n",
    "logistic_grid = GridSearchCV(logistic_smote_pipeline, logistic_params, cv=kf, scoring='accuracy', n_jobs=-1)\n",
    "forest_grid = GridSearchCV(rf_smote_pipeline, forest_params, cv=kf, scoring='accuracy', n_jobs=-1)\n",
    "svm_grid = GridSearchCV(svm_smote_pipeline, svm_params, cv=kf, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# Fit the grid search models\n",
    "logistic_grid.fit(X, y)\n",
    "print(f\"Best Logistic Regression Parameters: {logistic_grid.best_params_}\")\n",
    "print(f\"Logistic Regression Best CV Accuracy: {logistic_grid.best_score_:.4f}\")\n",
    "\n",
    "forest_grid.fit(X, y)\n",
    "print(f\"Best Random Forest Parameters: {forest_grid.best_params_}\")\n",
    "print(f\"Random Forest Best CV Accuracy: {forest_grid.best_score_:.4f}\")\n",
    "\n",
    "svm_grid.fit(X, y)\n",
    "print(f\"Best SVM Parameters: {svm_grid.best_params_}\")\n",
    "print(f\"SVM Best CV Accuracy: {svm_grid.best_score_:.4f}\")\n",
    "\n",
    "# XGBoost Classifier Model with Grid Search\n",
    "if USE_XGBOOST:\n",
    "    xgboost_params = {'classifier__n_estimators': [100, 200, 300], 'classifier__max_depth': [3, 5, 7]}\n",
    "    xgboost_grid = GridSearchCV(build_pipeline(XGBClassifier, {'use_label_encoder': False, 'eval_metric': 'mlogloss'}), xgboost_params, cv=kf, scoring='accuracy', n_jobs=-1)\n",
    "    xgboost_grid.fit(X, y)\n",
    "    print(f\"Best XGBoost Parameters: {xgboost_grid.best_params_}\")\n",
    "    print(f\"XGBoost Best CV Accuracy: {xgboost_grid.best_score_:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5975dc417b1cbe",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c42dd9e254ead329",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-13T14:36:30.027476Z",
     "start_time": "2023-12-13T14:36:30.011767Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Logistic Regression Model with Cross-Validation\n",
    "LOGISTIC_PARAMS = {\n",
    "    'multi_class': 'multinomial',\n",
    "    'C': 0.1,\n",
    "    'solver': 'lbfgs',\n",
    "    'max_iter': 1000\n",
    "}\n",
    "RANDOM_FOREST_PARAMS = {\n",
    "    'max_depth': 15,\n",
    "    'min_samples_split': 5,\n",
    "    'n_estimators': 200\n",
    "}\n",
    "SVM_PARAMS = {\n",
    "    'probability': True, \n",
    "    'C': 0.1,\n",
    "    'kernel': 'rbf'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cace1681aeaf0228",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### CV Without SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "62836a9c05cac59e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-13T14:36:43.454280Z",
     "start_time": "2023-12-13T14:36:31.333351Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression CV Accuracy: 0.5365 (+/- 0.0153)\n",
      "Random Forest CV Accuracy: 0.5351 (+/- 0.0130)\n",
      "SVM CV Accuracy: 0.5399 (+/- 0.0271)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "USE_XGBOOST = False\n",
    "\n",
    "# Sample DataFrame\n",
    "df = epl_df.copy()\n",
    "\n",
    "# Drop rows with NaN values and explicitly create a new copy\n",
    "df = df.dropna()\n",
    "\n",
    "# Encode the 'FTR' column\n",
    "target_column = 'FTR' # W, D, L becomes 0, 1, 2\n",
    "\n",
    "# Features and Target\n",
    "X = df.drop(target_column, axis=1)\n",
    "# X = df.drop(['HomeElo', 'AwayElo'], axis=1)\n",
    "X = scaler.fit_transform(X)\n",
    "y = df[target_column]\n",
    "\n",
    "# Define 5-fold cross-validation\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "\n",
    "logistic_model = LogisticRegression(**LOGISTIC_PARAMS)\n",
    "logistic_scores = cross_val_score(logistic_model, X, y, cv=kf, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# Random Forest Classifier Model with Cross-Validation\n",
    "random_forest_model = RandomForestClassifier(**RANDOM_FOREST_PARAMS)\n",
    "forest_scores = cross_val_score(random_forest_model, X, y, cv=kf, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# XGBoost Classifier Model with Cross-Validation\n",
    "if USE_XGBOOST:\n",
    "    from xgboost import XGBClassifier\n",
    "    xgboost_model = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
    "    xgboost_scores = cross_val_score(xgboost_model, X, y, cv=kf, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "\n",
    "# SVM Classifier Model with Cross-Validation\n",
    "svm_model = SVC(**SVM_PARAMS)\n",
    "svm_scores = cross_val_score(svm_model, X, y, cv=kf, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "\n",
    "# Displaying the results\n",
    "print(f\"Logistic Regression CV Accuracy: {logistic_scores.mean():.4f} (+/- {logistic_scores.std() * 2:.4f})\")\n",
    "print(f\"Random Forest CV Accuracy: {forest_scores.mean():.4f} (+/- {forest_scores.std() * 2:.4f})\")\n",
    "if USE_XGBOOST:\n",
    "    print(f\"XGBoost CV Accuracy: {xgboost_scores.mean():.4f} (+/- {xgboost_scores.std() * 2:.4f})\")\n",
    "print(f\"SVM CV Accuracy: {svm_scores.mean():.4f} (+/- {svm_scores.std() * 2:.4f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def523d8b5f7dd02",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### CV With SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5b9a905a33d28436",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-13T14:43:24.494980Z",
     "start_time": "2023-12-13T14:43:02.868450Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression CV Accuracy: 0.5062 (+/- 0.0197)\n",
      "Random Forest CV Accuracy: 0.5105 (+/- 0.0189)\n",
      "SVM CV Accuracy: 0.5121 (+/- 0.0258)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "USE_XGBOOST = False\n",
    "\n",
    "# Sample DataFrame\n",
    "df = epl_df.copy()\n",
    "\n",
    "# Drop rows with NaN values and explicitly create a new copy\n",
    "df = df.dropna()\n",
    "\n",
    "# Encode the 'FTR' column\n",
    "target_column = 'FTR' # W, D, L becomes 0, 1, 2\n",
    "\n",
    "# Features and Target\n",
    "X = df.drop(target_column, axis=1)\n",
    "# X = df.drop(['HomeElo', 'AwayElo'], axis=1)\n",
    "X = scaler.fit_transform(X)\n",
    "y = df[target_column]\n",
    "\n",
    "# Define 5-fold cross-validation\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "\n",
    "logistic_model = build_pipeline(LogisticRegression, LOGISTIC_PARAMS)\n",
    "logistic_scores = cross_val_score(logistic_model, X, y, cv=kf, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# Random Forest Classifier Model with Cross-Validation\n",
    "random_forest_model = build_pipeline(RandomForestClassifier, RANDOM_FOREST_PARAMS)\n",
    "forest_scores = cross_val_score(random_forest_model, X, y, cv=kf, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# XGBoost Classifier Model with Cross-Validation\n",
    "if USE_XGBOOST:\n",
    "    from xgboost import XGBClassifier\n",
    "    xgboost_model = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
    "    xgboost_scores = cross_val_score(xgboost_model, X, y, cv=kf, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "\n",
    "# SVM Classifier Model with Cross-Validation\n",
    "svm_model = build_pipeline(SVC, SVM_PARAMS)\n",
    "svm_scores = cross_val_score(svm_model, X, y, cv=kf, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "\n",
    "# Displaying the results\n",
    "print(f\"Logistic Regression CV Accuracy: {logistic_scores.mean():.4f} (+/- {logistic_scores.std() * 2:.4f})\")\n",
    "print(f\"Random Forest CV Accuracy: {forest_scores.mean():.4f} (+/- {forest_scores.std() * 2:.4f})\")\n",
    "if USE_XGBOOST:\n",
    "    print(f\"XGBoost CV Accuracy: {xgboost_scores.mean():.4f} (+/- {xgboost_scores.std() * 2:.4f})\")\n",
    "print(f\"SVM CV Accuracy: {svm_scores.mean():.4f} (+/- {svm_scores.std() * 2:.4f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10c3a55488a5504",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## classification report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a7c5bca8c3f5a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### with smote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "27bfddff6a873442",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-13T14:39:07.232939Z",
     "start_time": "2023-12-13T14:38:48.330395Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Logistic Regression\n",
      "Accuracy: 0.5115658362989324\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.58      0.55       336\n",
      "           1       0.30      0.37      0.33       274\n",
      "           2       0.69      0.54      0.61       514\n",
      "\n",
      "    accuracy                           0.51      1124\n",
      "   macro avg       0.50      0.50      0.49      1124\n",
      "weighted avg       0.54      0.51      0.52      1124\n",
      "\n",
      "Model: Random Forest\n",
      "Accuracy: 0.5284697508896797\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.59      0.54       336\n",
      "           1       0.31      0.27      0.29       274\n",
      "           2       0.65      0.63      0.64       514\n",
      "\n",
      "    accuracy                           0.53      1124\n",
      "   macro avg       0.49      0.49      0.49      1124\n",
      "weighted avg       0.52      0.53      0.52      1124\n",
      "Model: Support Vector Machine (SVM)\n",
      "Accuracy: 0.5169039145907474\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.61      0.57       336\n",
      "           1       0.31      0.39      0.34       274\n",
      "           2       0.68      0.53      0.59       514\n",
      "\n",
      "    accuracy                           0.52      1124\n",
      "   macro avg       0.51      0.51      0.50      1124\n",
      "weighted avg       0.55      0.52      0.53      1124\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import SMOTE  # Import SMOTE\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "df = epl_df.copy()\n",
    "\n",
    "# Specify the name of the target column\n",
    "target_column = 'FTR'  # Replace with the name of your target column\n",
    "\n",
    "# Features and Target\n",
    "X = df.drop(target_column, axis=1)\n",
    "X = scaler.fit_transform(X)\n",
    "y = df[target_column]  # Target\n",
    "\n",
    "# Splitting the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True)\n",
    "\n",
    "# Apply SMOTE\n",
    "smote = SMOTE()\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Create and train models on the SMOTE-augmented data\n",
    "logistic_model = LogisticRegression(**LOGISTIC_PARAMS)\n",
    "logistic_model.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "random_forest_model = RandomForestClassifier(**RANDOM_FOREST_PARAMS)\n",
    "random_forest_model.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "svm_model = SVC(**SVM_PARAMS)  # Ensure probability is set to True if needed\n",
    "svm_model.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "if USE_XGBOOST:\n",
    "    from xgboost import XGBClassifier, plot_importance\n",
    "    xgboost_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "    xgboost_model.fit(X_train_smote, y_train_smote)\n",
    "    plot_importance(xgboost_model)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Predicting on the test set\n",
    "y_pred = logistic_model.predict(X_test)\n",
    "\n",
    "# Evaluating the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred, zero_division=1)\n",
    "\n",
    "print(\"Model: Logistic Regression\")\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\\n\", report)\n",
    "\n",
    "y_pred = random_forest_model.predict(X_test)\n",
    "\n",
    "# Evaluating the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred, zero_division=1)\n",
    "print(\"Model: Random Forest\")\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\\n\", report)\n",
    "\n",
    "if USE_XGBOOST:\n",
    "    y_pred = xgboost_model.predict(X_test)\n",
    "\n",
    "    # Evaluating the model\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred, zero_division=1)\n",
    "\n",
    "    print(\"Model: XGBoost\")\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Classification Report:\\n\", report)\n",
    "\n",
    "# Predicting on the test set with SVM\n",
    "y_pred = svm_model.predict(X_test)\n",
    "\n",
    "# Evaluating the SVM model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred, zero_division=1)\n",
    "\n",
    "print(\"Model: Support Vector Machine (SVM)\")\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5efc6d8cb154ba",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### without smote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1abb84a4df3ee600",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-13T14:45:18.062427Z",
     "start_time": "2023-12-13T14:45:07.495600Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Logistic Regression\n",
      "Accuracy: 0.550711743772242\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.54      0.53       329\n",
      "           1       0.33      0.11      0.16       266\n",
      "           2       0.60      0.78      0.68       529\n",
      "\n",
      "    accuracy                           0.55      1124\n",
      "   macro avg       0.48      0.48      0.46      1124\n",
      "weighted avg       0.51      0.55      0.51      1124\n",
      "\n",
      "Model: Random Forest\n",
      "Accuracy: 0.5364768683274022\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.51      0.50       329\n",
      "           1       0.30      0.06      0.11       266\n",
      "           2       0.57      0.79      0.67       529\n",
      "\n",
      "    accuracy                           0.54      1124\n",
      "   macro avg       0.46      0.45      0.42      1124\n",
      "weighted avg       0.49      0.54      0.48      1124\n",
      "Model: Support Vector Machine (SVM)\n",
      "Accuracy: 0.5462633451957295\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.48      0.50       329\n",
      "           1       1.00      0.00      0.00       266\n",
      "           2       0.55      0.86      0.67       529\n",
      "\n",
      "    accuracy                           0.55      1124\n",
      "   macro avg       0.69      0.45      0.39      1124\n",
      "weighted avg       0.65      0.55      0.46      1124\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import SMOTE  # Import SMOTE\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "df = epl_df.copy()\n",
    "\n",
    "# Specify the name of the target column\n",
    "target_column = 'FTR'  # Replace with the name of your target column\n",
    "\n",
    "# Features and Target\n",
    "X = df.drop(target_column, axis=1)\n",
    "X = scaler.fit_transform(X)\n",
    "y = df[target_column]  # Target\n",
    "\n",
    "# Splitting the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True)\n",
    "\n",
    "\n",
    "# Create and train models on the SMOTE-augmented data\n",
    "logistic_model = LogisticRegression(**LOGISTIC_PARAMS)\n",
    "logistic_model.fit(X_train, y_train)\n",
    "\n",
    "random_forest_model = RandomForestClassifier(**RANDOM_FOREST_PARAMS)\n",
    "random_forest_model.fit(X_train, y_train)\n",
    "\n",
    "svm_model = SVC(**SVM_PARAMS)  # Ensure probability is set to True if needed\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "if USE_XGBOOST:\n",
    "    from xgboost import XGBClassifier, plot_importance\n",
    "    xgboost_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "    xgboost_model.fit(X_train, y_train)\n",
    "    plot_importance(xgboost_model)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Predicting on the test set\n",
    "y_pred = logistic_model.predict(X_test)\n",
    "\n",
    "# Evaluating the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred, zero_division=1)\n",
    "\n",
    "print(\"Model: Logistic Regression\")\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\\n\", report)\n",
    "\n",
    "y_pred = random_forest_model.predict(X_test)\n",
    "\n",
    "# Evaluating the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred, zero_division=1)\n",
    "print(\"Model: Random Forest\")\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\\n\", report)\n",
    "\n",
    "if USE_XGBOOST:\n",
    "    y_pred = xgboost_model.predict(X_test)\n",
    "\n",
    "    # Evaluating the model\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred, zero_division=1)\n",
    "\n",
    "    print(\"Model: XGBoost\")\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Classification Report:\\n\", report)\n",
    "\n",
    "# Predicting on the test set with SVM\n",
    "y_pred = svm_model.predict(X_test)\n",
    "\n",
    "# Evaluating the SVM model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred, zero_division=1)\n",
    "\n",
    "print(\"Model: Support Vector Machine (SVM)\")\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39375288a89ba0d",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "\n",
    "# Define the MLP class\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, layer_num, output_dim, hidden_dim, dropout_prob=0.4):\n",
    "        super(MLP, self).__init__()\n",
    "        layers = []\n",
    "\n",
    "        # Input layer\n",
    "        layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(nn.Dropout(dropout_prob))\n",
    "\n",
    "        # Hidden layers\n",
    "        for _ in range(layer_num - 2):\n",
    "            layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout_prob))\n",
    "\n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(hidden_dim, output_dim))\n",
    "\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, input):\n",
    "        for layer in self.layers:\n",
    "            input = layer(input)\n",
    "        return input\n",
    "\n",
    "def calculate_accuracy(y_true, y_pred):\n",
    "    predicted = torch.argmax(y_pred, dim=1)\n",
    "    correct = (predicted == y_true).float()\n",
    "    return correct.sum() / len(correct)\n",
    "\n",
    "# Function to evaluate the model on the test set\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    all_y_pred = []\n",
    "    all_y_true = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            y_pred_tensor = model(X_batch)\n",
    "            y_pred = torch.argmax(y_pred_tensor, dim=1)\n",
    "            all_y_pred.extend(y_pred.cpu().numpy())\n",
    "            all_y_true.extend(y_batch.cpu().numpy())\n",
    "\n",
    "    return all_y_true, all_y_pred\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Assume 'df' is your DataFrame\n",
    "df = epl_df.copy()\n",
    "\n",
    "# Preprocess DataFrame\n",
    "X = df.iloc[:, :-1].values  # All columns except last\n",
    "y = df.iloc[:, -1].values  # Last column\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Encode labels ('H', 'D', 'A') to integers\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Convert to PyTorch tensors (keep them on CPU for now)\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y_encoded, dtype=torch.long)\n",
    "\n",
    "# Create a custom dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "\n",
    "# Initial train-test split\n",
    "train_X, test_X, train_y, test_y = train_test_split(X_tensor, y_tensor, test_size=0.2, random_state=42)\n",
    "\n",
    "# Apply SMOTE to the training set to balance it\n",
    "smote = SMOTE(random_state=42)\n",
    "train_X_resampled, train_y_resampled = smote.fit_resample(train_X, train_y)\n",
    "\n",
    "# Further split the resampled train set into train and validation sets\n",
    "train_X_resampled, val_X, train_y_resampled, val_y = train_test_split(train_X_resampled, train_y_resampled, test_size=0.25, random_state=42) # 0.25 x 0.8 = 0.2\n",
    "\n",
    "# Create custom datasets with the resampled data\n",
    "train_dataset = CustomDataset(train_X_resampled, train_y_resampled)\n",
    "val_dataset = CustomDataset(val_X, val_y)\n",
    "test_dataset = CustomDataset(test_X, test_y)\n",
    "\n",
    "# Create DataLoaders for train, validation, and test sets\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "best_accuracy = 0\n",
    "best_accuracy_report = None\n",
    "\n",
    "# Model Parameters\n",
    "input_dim = X.shape[1]  # Number of features\n",
    "output_dim = 3  # For three categories\n",
    "layer_num = 5  # Number of layers\n",
    "hidden_dim = 40  # Number of neurons in hidden layer\n",
    "learning_rate = 0.001\n",
    "epochs = 30\n",
    "weight_decay = 1e-2  # L2 regularization factor\n",
    "\n",
    "for run in range(100):\n",
    "  model = MLP(input_dim, layer_num, output_dim, hidden_dim).to(device)\n",
    "  optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "  # Training and Validation Loop\n",
    "  for epoch in range(epochs):\n",
    "\n",
    "    # Training Phase\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validation Phase\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    total_val_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in val_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "  # Evaluate the model\n",
    "  y_test_np, y_pred = evaluate_model(model, test_loader)\n",
    "\n",
    "  # Calculate accuracy and F1 score\n",
    "  accuracy = accuracy_score(y_test_np, y_pred)\n",
    "  report = classification_report(y_test_np, y_pred, zero_division=1)\n",
    "\n",
    "  # Update best scores and reports\n",
    "  if accuracy > best_accuracy:\n",
    "      best_accuracy = accuracy\n",
    "      best_accuracy_report = report\n",
    "\n",
    "print(\"Best Accuracy Report:\")\n",
    "print(best_accuracy_report)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
